{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guRrcfCKVrev"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n",
        "      raw_train_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n",
        "      raw_test_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n",
        "      raw_val_data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.DataFrame(raw_train_data)\n",
        "test_data=pd.DataFrame(raw_test_data)\n",
        "val_data=pd.DataFrame(raw_val_data)"
      ],
      "metadata": {
        "id": "xgyb32ycVuv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([train_data, val_data], axis=0)\n",
        "df_all = pd.concat([df_all, test_data], axis=0)"
      ],
      "metadata": {
        "id": "UT-AYt7sVvwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Assuming df_all is your training dataset and df_test is your test dataset\n",
        "df_all = train_data\n",
        "df_test = test_data\n",
        "\n",
        "# Create a new column 'target' in the training data based on your criteria (e.g., 'bipolar')\n",
        "df_all['target'] = df_all['diseases'].apply(lambda x: 1 if 'bipolar' in x else 0)\n",
        "df_test['target'] = df_test['diseases'].apply(lambda x: 1 if 'bipolar' in x else 0)  # If you have labels in test data\n",
        "\n",
        "# Define keywords related to your target condition\n",
        "bipolar_keywords = [\n",
        "    'hyper', 'irritable', 'self-confident', 'talkative', 'sleep', 'thoughts',\n",
        "    'distracted', 'energy', 'active', 'social', 'sex', 'excessive', 'foolish', 'risky', 'spending'\n",
        "]\n",
        "\n",
        "# Function to count the occurrences of bipolar-related keywords in the posts\n",
        "def count_bipolar_keywords(posts):\n",
        "    # Joining posts if they are in list format\n",
        "    posts = ' '.join(posts) if isinstance(posts, list) else posts\n",
        "    return sum(keyword in posts.lower() for keyword in bipolar_keywords)\n",
        "\n",
        "# Apply the function to the training and test data\n",
        "df_all['bipolar_keyword_count'] = df_all['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).apply(count_bipolar_keywords)\n",
        "df_test['bipolar_keyword_count'] = df_test['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).apply(count_bipolar_keywords)\n",
        "\n",
        "# Prepare the data for training\n",
        "X_train = df_all['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "y_train = df_all['target']\n",
        "X_test = df_test['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "y_test = df_test['target']\n",
        "\n",
        "# Define the pipeline with TF-IDF and Logistic Regression, handling class imbalance with SMOTE\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Train the model on the entire training set\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Perform bootstrap sampling 100 times on the test set\n",
        "n_bootstraps = 100\n",
        "f1_scores = []\n",
        "sensitivity_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "for _ in range(n_bootstraps):\n",
        "    # Bootstrap sample the test data\n",
        "    X_boot, y_boot = resample(X_test, y_test, random_state=_)\n",
        "\n",
        "    # Predict on the bootstrap sample of the test set\n",
        "    y_pred = pipeline.predict(X_boot)\n",
        "\n",
        "    # Compute metrics for this bootstrap sample\n",
        "    f1_scores.append(f1_score(y_boot, y_pred))\n",
        "    sensitivity_scores.append(recall_score(y_boot, y_pred))\n",
        "    specificity_scores.append(recall_score(y_boot, y_pred, pos_label=0))\n",
        "\n",
        "# Calculate average and standard deviation for each metric\n",
        "average_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "average_sensitivity = np.mean(sensitivity_scores)\n",
        "std_sensitivity = np.std(sensitivity_scores)\n",
        "average_specificity = np.mean(specificity_scores)\n",
        "std_specificity = np.std(specificity_scores)\n",
        "\n",
        "# Print average metrics with standard deviations\n",
        "print(f\"F1 Score: {average_f1:.3f} ± {std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {average_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {average_specificity:.3f} ± {std_specificity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OYw6lF2ZL3t",
        "outputId": "4a0b57f2-42bc-47c2-c9ea-27c5c6e6545c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.544 ± 0.033\n",
            "Sensitivity: 0.594 ± 0.040\n",
            "Specificity: 0.964 ± 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, classification_report\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Define bipolar keywords related to the assessment scale\n",
        "bipolar_keywords = [\n",
        "    'hyper', 'irritable', 'self-confident', 'talkative', 'sleep', 'thoughts',\n",
        "    'distracted', 'energy', 'active', 'social', 'sex', 'excessive', 'foolish', 'risky', 'spending'\n",
        "]\n",
        "\n",
        "# Function to count the occurrences of bipolar-related keywords in the posts\n",
        "def count_bipolar_keywords(posts):\n",
        "    # Joining posts if they are in list format\n",
        "    posts = ' '.join(posts) if isinstance(posts, list) else posts\n",
        "    return sum(keyword in posts.lower() for keyword in bipolar_keywords)\n",
        "\n",
        "# Convert each entry in 'selected_posts' to a single string\n",
        "df_all['selected_posts'] = df_all['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Apply the function to create a new feature based on keyword counts\n",
        "df_all['bipolar_keyword_count'] = df_all['selected_posts'].apply(count_bipolar_keywords)\n",
        "\n",
        "# Use the 'diseases' column to define the target label\n",
        "df_all['target'] = df_all['diseases'].apply(lambda x: 1 if 'bipolar' in x else 0)\n",
        "\n",
        "# Prepare the data for training and evaluation\n",
        "X = df_all[['selected_posts', 'bipolar_keyword_count']]\n",
        "y = df_all['target']\n",
        "\n",
        "# Define the pipeline with a ColumnTransformer to manage both TF-IDF and keyword count features\n",
        "pipeline = ImbPipeline([\n",
        "    ('preprocessing', ColumnTransformer([\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english'), 'selected_posts'),\n",
        "        ('keywords', 'passthrough', ['bipolar_keyword_count'])\n",
        "    ], remainder='drop')),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Initialize 5-fold stratified cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "sensitivity_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "# Perform cross-validation on the entire dataset\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict on the validation fold\n",
        "    y_pred = pipeline.predict(X_val_fold)\n",
        "\n",
        "    # Compute metrics\n",
        "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
        "    sensitivity_scores.append(recall_score(y_val_fold, y_pred))\n",
        "    specificity_scores.append(recall_score(y_val_fold, y_pred, pos_label=0))\n",
        "\n",
        "# Calculate average and standard deviation for each metric\n",
        "average_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "average_sensitivity = np.mean(sensitivity_scores)\n",
        "std_sensitivity = np.std(sensitivity_scores)\n",
        "average_specificity = np.mean(specificity_scores)\n",
        "std_specificity = np.std(specificity_scores)\n",
        "\n",
        "# Print average metrics with standard deviations\n",
        "print(f\"F1 Score: {average_f1:.3f} ± {std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {average_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {average_specificity:.3f} ± {std_specificity:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GJFictv8Ydcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c60ff1-ee5b-403f-ffc5-c2dac12d34dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.308 ± 0.025\n",
            "Sensitivity: 0.249 ± 0.025\n",
            "Specificity: 0.980 ± 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, classification_report\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Load your dataset (Assuming df_all is already loaded and processed as described before)\n",
        "# df_all = pd.read_csv('path_to_your_train_data.csv')  # Uncomment and update with your dataset path if needed\n",
        "\n",
        "# Define bipolar keywords related to the assessment scale\n",
        "bipolar_keywords = [\n",
        "    'hyper', 'irritable', 'self-confident', 'talkative', 'sleep', 'thoughts',\n",
        "    'distracted', 'energy', 'active', 'social', 'sex', 'excessive', 'foolish', 'risky', 'spending'\n",
        "]\n",
        "\n",
        "# Function to count the occurrences of bipolar-related keywords in the posts\n",
        "def count_bipolar_keywords(posts):\n",
        "    # Joining posts if they are in list format\n",
        "    posts = ' '.join(posts) if isinstance(posts, list) else posts\n",
        "    return sum(keyword in posts.lower() for keyword in bipolar_keywords)\n",
        "\n",
        "# Convert each entry in 'selected_posts' to a single string\n",
        "df_all['selected_posts'] = df_all['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Apply the function to create a new feature based on keyword counts\n",
        "df_all['bipolar_keyword_count'] = df_all['selected_posts'].apply(count_bipolar_keywords)\n",
        "\n",
        "# Use the 'diseases' column to define the target label\n",
        "df_all['target'] = df_all['diseases'].apply(lambda x: 1 if 'bipolar' in x else 0)\n",
        "\n",
        "# Prepare the data for training and evaluation\n",
        "X = df_all[['selected_posts', 'bipolar_keyword_count']]\n",
        "y = df_all['target']\n",
        "\n",
        "# Define the pipeline with a ColumnTransformer to manage both CountVectorizer and keyword count features\n",
        "pipeline = ImbPipeline([\n",
        "    ('preprocessing', ColumnTransformer([\n",
        "        ('count_vectorizer', CountVectorizer(stop_words='english'), 'selected_posts'),\n",
        "        ('keywords', 'passthrough', ['bipolar_keyword_count'])\n",
        "    ], remainder='drop')),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Initialize 5-fold stratified cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "sensitivity_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "# Perform cross-validation on the entire dataset\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict on the validation fold\n",
        "    y_pred = pipeline.predict(X_val_fold)\n",
        "\n",
        "    # Compute metrics\n",
        "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
        "    sensitivity_scores.append(recall_score(y_val_fold, y_pred))\n",
        "    specificity_scores.append(recall_score(y_val_fold, y_pred, pos_label=0))\n",
        "\n",
        "# Calculate average and standard deviation for each metric\n",
        "average_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "average_sensitivity = np.mean(sensitivity_scores)\n",
        "std_sensitivity = np.std(sensitivity_scores)\n",
        "average_specificity = np.mean(specificity_scores)\n",
        "std_specificity = np.std(specificity_scores)\n",
        "\n",
        "# Print average metrics with standard deviations\n",
        "print(f\"F1 Score: {average_f1:.3f} ± {std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {average_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {average_specificity:.3f} ± {std_specificity:.3f}\")\n"
      ],
      "metadata": {
        "id": "N7lSpJmhLu02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b0c16d-d892-487b-dabf-7c2792bba9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.304 ± 0.020\n",
            "Sensitivity: 0.219 ± 0.020\n",
            "Specificity: 0.988 ± 0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, recall_score, classification_report\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Load your dataset (Assuming df_all is already loaded and processed as described before)\n",
        "# df_all = pd.read_csv('path_to_your_train_data.csv')  # Uncomment and update with your dataset path if needed\n",
        "\n",
        "# Define bipolar keywords related to the assessment scale\n",
        "bipolar_keywords = [\n",
        "    'hyper', 'irritable', 'self-confident', 'talkative', 'sleep', 'thoughts',\n",
        "    'distracted', 'energy', 'active', 'social', 'sex', 'excessive', 'foolish', 'risky', 'spending'\n",
        "]\n",
        "\n",
        "# Function to count the occurrences of bipolar-related keywords in the posts\n",
        "def count_bipolar_keywords(posts):\n",
        "    # Joining posts if they are in list format\n",
        "    posts = ' '.join(posts) if isinstance(posts, list) else posts\n",
        "    return sum(keyword in posts.lower() for keyword in bipolar_keywords)\n",
        "\n",
        "# Convert each entry in 'selected_posts' to a single string\n",
        "df_all['selected_posts'] = df_all['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Apply the function to create a new feature based on keyword counts\n",
        "df_all['bipolar_keyword_count'] = df_all['selected_posts'].apply(count_bipolar_keywords)\n",
        "\n",
        "# Use the 'diseases' column to define the target label\n",
        "df_all['target'] = df_all['diseases'].apply(lambda x: 1 if 'bipolar' in x else 0)\n",
        "\n",
        "# Prepare the data for training and evaluation\n",
        "X = df_all[['selected_posts', 'bipolar_keyword_count']]\n",
        "y = df_all['target']\n",
        "\n",
        "# Define the pipeline with a ColumnTransformer to manage both CountVectorizer and keyword count features\n",
        "pipeline = ImbPipeline([\n",
        "    ('preprocessing', ColumnTransformer([\n",
        "        ('count_vectorizer', CountVectorizer(stop_words='english'), 'selected_posts'),\n",
        "        ('keywords', 'passthrough', ['bipolar_keyword_count'])\n",
        "    ], remainder='drop')),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Initialize 5-fold stratified cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "sensitivity_scores = []\n",
        "specificity_scores = []\n",
        "\n",
        "# Perform cross-validation on the entire dataset\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict on the validation fold\n",
        "    y_pred = pipeline.predict(X_val_fold)\n",
        "\n",
        "    # Compute metrics\n",
        "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
        "    sensitivity_scores.append(recall_score(y_val_fold, y_pred))\n",
        "    specificity_scores.append(recall_score(y_val_fold, y_pred, pos_label=0))\n",
        "\n",
        "# Calculate average and standard deviation for each metric\n",
        "average_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "average_sensitivity = np.mean(sensitivity_scores)\n",
        "std_sensitivity = np.std(sensitivity_scores)\n",
        "average_specificity = np.mean(specificity_scores)\n",
        "std_specificity = np.std(specificity_scores)\n",
        "\n",
        "# Print average metrics with standard deviations\n",
        "print(f\"F1 Score: {average_f1:.3f} ± {std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {average_sensitivity:.3f} ± {std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {average_specificity:.3f} ± {std_specificity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ5x2gjYIsn8",
        "outputId": "04043294-b366-47b2-a0a0-cd1c28e5eb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.480 ± 0.031\n",
            "Sensitivity: 0.435 ± 0.033\n",
            "Specificity: 0.980 ± 0.001\n"
          ]
        }
      ]
    }
  ]
}