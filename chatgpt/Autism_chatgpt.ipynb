{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-gjR4MKaWnw"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n",
        "      raw_train_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n",
        "      raw_test_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n",
        "      raw_val_data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.DataFrame(raw_train_data)\n",
        "test_data=pd.DataFrame(raw_test_data)\n",
        "val_data=pd.DataFrame(raw_val_data)"
      ],
      "metadata": {
        "id": "JQZXva4Ja7Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([train_data, val_data], axis=0)\n",
        "df_all = pd.concat([df_all, test_data], axis=0)"
      ],
      "metadata": {
        "id": "MFZq5PIDa8JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, confusion_matrix, make_scorer\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Load and prepare the dataset\n",
        "data = df_all\n",
        "data['combined_posts'] = data['selected_posts'].apply(lambda posts: ' '.join(posts))\n",
        "data['has_autism'] = data['diseases'].apply(lambda diseases: 1 if 'autism' in diseases else 0)\n",
        "\n",
        "# Define features and target labels\n",
        "X = data['combined_posts']\n",
        "y = data['has_autism']\n",
        "\n",
        "# Define a pipeline for text processing and classification\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define custom scorer for specificity\n",
        "def specificity_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "    else:\n",
        "        specificity = np.nan\n",
        "    return specificity\n",
        "\n",
        "specificity_scorer = make_scorer(specificity_score, greater_is_better=True)\n",
        "\n",
        "# Perform 5-fold cross-validation and calculate metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=kf)\n",
        "sensitivity_scores = cross_val_score(pipeline, X, y, cv=kf)\n",
        "specificity_scores = cross_val_score(pipeline, X, y, cv=kf, scoring=specificity_scorer)\n",
        "\n",
        "# Calculate mean and standard deviation for each metric\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "sensitivity_mean, sensitivity_std = np.mean(sensitivity_scores), np.std(sensitivity_scores)\n",
        "specificity_mean, specificity_std = np.mean(specificity_scores), np.std(specificity_scores)\n",
        "\n",
        "print(\"Metrics after 5-fold cross-validation:\")\n",
        "print(f\"F1 Score: {f1_mean:.3f} ± {f1_std:.3f}\")\n",
        "print(f\"Sensitivity: {sensitivity_mean:.3f} ± {sensitivity_std:.3f}\")\n",
        "print(f\"Specificity: {specificity_mean:.3f} ± {specificity_std:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dekOCkIQ-M7K",
        "outputId": "160f1231-d91c-4f26-e5f9-b45ed20d085e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics after 5-fold cross-validation:\n",
            "F1 Score: 0.973 ± 0.002\n",
            "Sensitivity: 0.973 ± 0.002\n",
            "Specificity: 1.000 ± 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZDwFBxrIGC4",
        "outputId": "b61072b7-7db3-4f4a-d6d1-e74986deb293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26605"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, confusion_matrix, make_scorer\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Load and prepare the dataset\n",
        "data = df_all\n",
        "data['combined_posts'] = data['selected_posts'].apply(lambda posts: ' '.join(posts))\n",
        "data['has_autism'] = data['diseases'].apply(lambda diseases: 1 if 'autism' in diseases else 0)\n",
        "\n",
        "# Define features and target labels\n",
        "X = data['combined_posts']\n",
        "y = data['has_autism']\n",
        "\n",
        "# Define a pipeline for text processing and classification\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define custom scorer for specificity\n",
        "def specificity_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "    else:\n",
        "        specificity = np.nan\n",
        "    return specificity\n",
        "\n",
        "specificity_scorer = make_scorer(specificity_score, greater_is_better=True)\n",
        "\n",
        "# Perform 5-fold cross-validation and calculate metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=kf)\n",
        "sensitivity_scores = cross_val_score(pipeline, X, y, cv=kf)\n",
        "specificity_scores = cross_val_score(pipeline, X, y, cv=kf, scoring=specificity_scorer)\n",
        "\n",
        "# Calculate mean and standard deviation for each metric\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "sensitivity_mean, sensitivity_std = np.mean(sensitivity_scores), np.std(sensitivity_scores)\n",
        "specificity_mean, specificity_std = np.mean(specificity_scores), np.std(specificity_scores)\n",
        "\n",
        "print(\"Metrics after 5-fold cross-validation:\")\n",
        "print(f\"F1 Score: {f1_mean:.3f} ± {f1_std:.3f}\")\n",
        "print(f\"Sensitivity: {sensitivity_mean:.3f} ± {sensitivity_std:.3f}\")\n",
        "print(f\"Specificity: {specificity_mean:.3f} ± {specificity_std:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT-CjHQKiib0",
        "outputId": "ef6ea458-1d98-4007-f0ab-7b4204456066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics after 5-fold cross-validation:\n",
            "F1 Score: 0.883 ± 0.005\n",
            "Sensitivity: 0.902 ± 0.003\n",
            "Specificity: 0.987 ± 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autism_keywords = {\n",
        "    'item_1': ['social', 'group activities', 'others'],\n",
        "    'item_2': ['routine', 'same way', 'repetition'],\n",
        "    'item_3': ['imagine', 'picture in mind', 'visualization'],\n",
        "    'item_4': ['absorbed', 'focus', 'lose sight'],\n",
        "    'item_5': ['small sounds', 'notices sounds'],\n",
        "    'item_6': ['car number plates', 'strings of information'],\n",
        "    'item_7': ['impolite', 'polite'],\n",
        "    'item_8': ['reading a story', 'imagine characters'],\n",
        "    'item_9': ['fascinated by dates'],\n",
        "    'item_10': ['social group', 'track of conversations'],\n",
        "    'item_11': ['social situations', 'easy'],\n",
        "    'item_12': ['notice details'],\n",
        "    'item_13': ['library', 'party'],\n",
        "    'item_14': ['making up stories', 'creative writing'],\n",
        "    'item_15': ['drawn to people', 'interested in people'],\n",
        "    'item_16': ['strong interests', 'upset if cannot pursue'],\n",
        "    'item_17': ['social chit-chat', 'small talk'],\n",
        "    'item_18': ['talks', 'get a word in edgeways'],\n",
        "    'item_19': ['fascinated by numbers'],\n",
        "    'item_20': ['reading a story', 'work out intentions'],\n",
        "    'item_21': ['enjoy reading fiction'],\n",
        "    'item_22': ['hard to make new friends'],\n",
        "    'item_23': ['notices patterns'],\n",
        "    'item_24': ['theatre', 'museum'],\n",
        "    'item_25': ['daily routine', 'disturbed'],\n",
        "    'item_26': ['keep a conversation going'],\n",
        "    'item_27': ['read between the lines'],\n",
        "    'item_28': ['whole picture', 'small details'],\n",
        "    'item_29': ['remembering phone numbers'],\n",
        "    'item_30': ['notice small changes', 'appearance'],\n",
        "    'item_31': ['tell if someone is bored'],\n",
        "    'item_32': ['do more than one thing', 'multitask'],\n",
        "    'item_33': ['talk on the phone', 'turn to speak'],\n",
        "    'item_34': ['doing things spontaneously'],\n",
        "    'item_35': ['last to understand the joke'],\n",
        "    'item_36': ['work out what someone is thinking', 'looking at their face'],\n",
        "    'item_37': ['switch back quickly', 'interruption'],\n",
        "    'item_38': ['good at social chit-chat'],\n",
        "    'item_39': ['going on and on about the same thing'],\n",
        "    'item_40': ['playing games', 'pretending'],\n",
        "    'item_41': ['collect information', 'categories of things'],\n",
        "    'item_42': ['imagine being someone else'],\n",
        "    'item_43': ['plan activities carefully'],\n",
        "    'item_44': ['enjoys social occasions'],\n",
        "    'item_45': ['work out people’s intentions'],\n",
        "    'item_46': ['new situations', 'anxious'],\n",
        "    'item_47': ['enjoys meeting new people'],\n",
        "    'item_48': ['good diplomat'],\n",
        "    'item_49': ['remembering dates of birth'],\n",
        "    'item_50': ['play games with children', 'pretending'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "2w47z4ve1T8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load and prepare the training data\n",
        "train_data['combined_posts'] = train_data['selected_posts'].apply(lambda posts: ' '.join(posts))\n",
        "\n",
        "# Advanced text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "train_data['processed_posts'] = train_data['combined_posts'].apply(preprocess_text)\n",
        "\n",
        "# Adjust the scoring function to be more sensitive\n",
        "def score_autism(posts, keywords):\n",
        "    scores = []\n",
        "    for post in posts:\n",
        "        score = 0\n",
        "        for key, value in keywords.items():\n",
        "            if any(word in post for word in value):\n",
        "                score += 1\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "train_data['autism_score'] = score_autism(train_data['processed_posts'], autism_keywords)\n",
        "train_data['has_autism'] = train_data['diseases'].apply(lambda x: 1 if 'autism' in x else 0)\n",
        "train_data['post_length'] = train_data['combined_posts'].apply(len)\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data['combined_posts'] = test_data['selected_posts'].apply(lambda posts: ' '.join(posts))\n",
        "test_data['processed_posts'] = test_data['combined_posts'].apply(preprocess_text)\n",
        "test_data['autism_score'] = score_autism(test_data['processed_posts'], autism_keywords)\n",
        "test_data['has_autism'] = test_data['diseases'].apply(lambda x: 1 if 'autism' in x else 0)\n",
        "test_data['post_length'] = test_data['combined_posts'].apply(len)\n",
        "\n",
        "# Define features and target labels for training\n",
        "X_train = train_data[['processed_posts', 'post_length', 'autism_score']]\n",
        "y_train = train_data['has_autism']\n",
        "\n",
        "# Define features for testing\n",
        "X_test = test_data[['processed_posts', 'post_length', 'autism_score']]\n",
        "y_test = test_data['has_autism']\n",
        "\n",
        "# Preprocessing for numerical and text data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', TfidfVectorizer(max_features=1000), 'processed_posts'),\n",
        "        ('num', StandardScaler(), ['post_length', 'autism_score'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define and train the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=200, max_depth=10))\n",
        "])\n",
        "\n",
        "# Train the model on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Function to perform bootstrapped sampling and calculate metrics on the test set\n",
        "def bootstrap_metrics(X, y, model, n_iterations=100, sample_size=None):\n",
        "    f1_scores = []\n",
        "    sensitivities = []  # Recall\n",
        "    specificities = []\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        X_sample, y_sample = resample(X, y, n_samples=sample_size)\n",
        "        y_pred = model.predict(X_sample)\n",
        "\n",
        "        f1_scores.append(f1_score(y_sample, y_pred, average='weighted'))\n",
        "        sensitivities.append(recall_score(y_sample, y_pred, average='weighted'))\n",
        "\n",
        "        # Calculate specificity using binary assumption; adjust as needed for multi-class\n",
        "        cm = confusion_matrix(y_sample, y_pred, labels=np.unique(y_sample))\n",
        "        if cm.shape == (2, 2):  # binary case\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "        else:  # multi-class or other issues\n",
        "            specificity = np.nan  # or handle differently\n",
        "        specificities.append(specificity)\n",
        "\n",
        "    return {\n",
        "        'f1_score': (np.mean(f1_scores), np.std(f1_scores)),\n",
        "        'sensitivity': (np.mean(sensitivities), np.std(sensitivities)),\n",
        "        'specificity': (np.mean(specificities), np.std(specificities))\n",
        "    }\n",
        "\n",
        "# Perform bootstrapped evaluation on the test set\n",
        "metrics = bootstrap_metrics(X_test, y_test, pipeline, n_iterations=100)\n",
        "print(\"Metrics after bootstrap sampling on test set for autism detection:\")\n",
        "print(f\"F1 Score: {metrics['f1_score'][0]:.3f} ± {metrics['f1_score'][1]:.3f}\")\n",
        "print(f\"Sensitivity: {metrics['sensitivity'][0]:.3f} ± {metrics['sensitivity'][1]:.3f}\")\n",
        "print(f\"Specificity: {metrics['specificity'][0]:.3f} ± {metrics['specificity'][1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1gAVtiyAX4p",
        "outputId": "f26f8437-93d8-4b76-d304-5c9ca14e9905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics after bootstrap sampling on test set for autism detection:\n",
            "F1 Score: 0.959 ± 0.004\n",
            "Sensitivity: 0.972 ± 0.003\n",
            "Specificity: 1.000 ± 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, confusion_matrix, make_scorer\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load and prepare the data\n",
        "data = df_all\n",
        "data['combined_posts'] = data['selected_posts'].apply(lambda posts: ' '.join(posts))\n",
        "\n",
        "# Advanced text preprocessing\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "data['processed_posts'] = data['combined_posts'].apply(preprocess_text)\n",
        "\n",
        "# Adjust the scoring function to be more sensitive\n",
        "def score_autism(posts, keywords):\n",
        "    scores = []\n",
        "    for post in posts:\n",
        "        score = 0\n",
        "        for key, value in keywords.items():\n",
        "            if any(word in post for word in value):\n",
        "                score += 1\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "data['autism_score'] = score_autism(data['processed_posts'], autism_keywords)\n",
        "data['has_autism'] = data['diseases'].apply(lambda x: 1 if 'autism' in x else 0)\n",
        "data['post_length'] = data['combined_posts'].apply(len)\n",
        "\n",
        "# Define features and target labels\n",
        "X = data[['processed_posts', 'post_length', 'autism_score']]\n",
        "y = data['has_autism']\n",
        "\n",
        "# Preprocessing for numerical and text data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', TfidfVectorizer(max_features=1000), 'processed_posts'),\n",
        "        ('num', StandardScaler(), ['post_length', 'autism_score'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define and train the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=200, max_depth=10))\n",
        "])\n",
        "\n",
        "# Define custom scorer for specificity\n",
        "def specificity_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "    else:\n",
        "        specificity = np.nan\n",
        "    return specificity\n",
        "\n",
        "specificity_scorer = make_scorer(specificity_score, greater_is_better=True)\n",
        "\n",
        "# Perform 5-fold cross-validation and calculate metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='f1_weighted')\n",
        "sensitivity_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='recall_weighted')\n",
        "specificity_scores = cross_val_score(pipeline, X, y, cv=kf, scoring=specificity_scorer)\n",
        "\n",
        "# Calculate mean and standard deviation for each metric\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "sensitivity_mean, sensitivity_std = np.mean(sensitivity_scores), np.std(sensitivity_scores)\n",
        "specificity_mean, specificity_std = np.mean(specificity_scores), np.std(specificity_scores)\n",
        "\n",
        "print(\"Metrics after 5-fold cross-validation for autism detection:\")\n",
        "print(f\"F1 Score: {f1_mean:.3f} ± {f1_std:.3f}\")\n",
        "print(f\"Sensitivity: {sensitivity_mean:.3f} ± {sensitivity_std:.3f}\")\n",
        "print(f\"Specificity: {specificity_mean:.3f} ± {specificity_std:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AItcLSyAO1N-",
        "outputId": "ce3236cb-aee9-4b28-892d-da6102198499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics after 5-fold cross-validation for autism detection:\n",
            "F1 Score: 0.960 ± 0.003\n",
            "Sensitivity: 0.973 ± 0.002\n",
            "Specificity: 1.000 ± 0.000\n"
          ]
        }
      ]
    }
  ]
}