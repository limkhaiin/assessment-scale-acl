{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"DbhjEeq0Ej4i","executionInfo":{"status":"ok","timestamp":1717534458279,"user_tz":600,"elapsed":26065,"user":{"displayName":"Khai-in Lim","userId":"05224726267173946597"}}},"outputs":[],"source":["import pickle\n","with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n","      raw_train_data = pickle.load(f)\n","with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n","      raw_test_data = pickle.load(f)\n","with open('/content/drive/MyDrive/Colab Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n","      raw_val_data = pickle.load(f)"]},{"cell_type":"code","source":["import pandas as pd\n","train_data=pd.DataFrame(raw_train_data)\n","test_data=pd.DataFrame(raw_test_data)\n","val_data=pd.DataFrame(raw_val_data)"],"metadata":{"id":"m0Bn0WGhExYf","executionInfo":{"status":"ok","timestamp":1717560712468,"user_tz":600,"elapsed":267,"user":{"displayName":"Khai-in Lim","userId":"05224726267173946597"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["df_all = pd.concat([train_data, val_data], axis=0)\n","df_all = pd.concat([df_all, test_data], axis=0)"],"metadata":{"id":"SQ6igSn1Eyc_","executionInfo":{"status":"ok","timestamp":1717562228161,"user_tz":600,"elapsed":234,"user":{"displayName":"Khai-in Lim","userId":"05224726267173946597"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["|\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, recall_score, precision_score\n","from sklearn.utils import resample\n","\n","# Load your datasets\n","train = train_data\n","test = test_data\n","\n","\n","# Function to convert a string representation of a list into a single string of posts\n","def convert_posts(posts):\n","    if isinstance(posts, str):\n","        try:\n","            # Attempt to evaluate the string as a list\n","            posts_list = eval(posts)\n","            if isinstance(posts_list, list):\n","                return ' '.join(posts_list)\n","        except:\n","            return posts\n","    return ' '.join(posts)\n","\n","# Convert the list of posts into a single string for each entry\n","train['selected_posts'] = train['selected_posts'].apply(convert_posts)\n","test['selected_posts'] = test['selected_posts'].apply(convert_posts)\n","\n","# Extract features using TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n","X_train = vectorizer.fit_transform(train['selected_posts'])\n","X_test = vectorizer.transform(test['selected_posts'])\n","\n","# Function to check if 'adhd' is in the diseases list\n","def check_adhd(diseases):\n","    # Check if 'adhd' is in the list of diseases\n","    return 'adhd' in [disease.lower() for disease in diseases]\n","\n","# Label the training and test data for ADHD\n","train['has_adhd'] = train['diseases'].apply(check_adhd)\n","test['has_adhd'] = test['diseases'].apply(check_adhd)\n","\n","# Train a classifier\n","classifier = LogisticRegression()\n","classifier.fit(X_train, train['has_adhd'])\n","\n","# Predict probabilities on the test data\n","test['adhd_probability'] = classifier.predict_proba(X_test)[:, 1]\n","\n","# Convert probabilities to ADHD ratings based on thresholds\n","def probability_to_rating(prob):\n","    if prob >= 0.75:\n","        return 'Very much'\n","    elif prob >= 0.5:\n","        return 'Pretty much'\n","    elif prob >= 0.25:\n","        return 'Somewhat'\n","    else:\n","        return 'Not at all'\n","\n","test['adhd_rating'] = test['adhd_probability'].apply(probability_to_rating)\n","\n","# Function to convert ratings to binary predictions for metrics calculation\n","def rating_to_binary(rating):\n","    return rating in ['Very much', 'Pretty much']\n","\n","# Convert ratings to binary predictions\n","test['predicted_adhd'] = test['adhd_rating'].apply(rating_to_binary)\n","\n","# Bootstrap sampling to estimate metrics and their standard deviations\n","def bootstrap_metrics(data, n_iterations=100):\n","    f1_scores, sensitivities, specificities = [], [], []\n","    for _ in range(n_iterations):\n","        sample = resample(data)\n","        f1, sensitivity, specificity = compute_metrics(sample['has_adhd'], sample['predicted_adhd'])\n","        f1_scores.append(f1)\n","        sensitivities.append(sensitivity)\n","        specificities.append(specificity)\n","\n","    return {\n","        \"f1_mean\": np.mean(f1_scores),\n","        \"f1_std\": np.std(f1_scores),\n","        \"sensitivity_mean\": np.mean(sensitivities),\n","        \"sensitivity_std\": np.std(sensitivities),\n","        \"specificity_mean\": np.mean(specificities),\n","        \"specificity_std\": np.std(specificities)\n","    }\n","\n","# Function to compute metrics\n","def compute_metrics(actual, predicted):\n","    f1 = f1_score(actual, predicted)\n","    sensitivity = recall_score(actual, predicted)\n","    specificity = recall_score(actual, predicted, pos_label=False)\n","    return f1, sensitivity, specificity\n","\n","# Compute and print the metrics using bootstrapped sampling\n","metrics = bootstrap_metrics(test)\n","print(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgVRr85kHtBH","executionInfo":{"status":"ok","timestamp":1717485800891,"user_tz":600,"elapsed":6909,"user":{"displayName":"Khai-in Lim","userId":"05224726267173946597"}},"outputId":"55766d4b-b0e5-41be-e151-787b25e36234"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'f1_mean': 0.3933247814520313, 'f1_std': 0.03201271326899809, 'sensitivity_mean': 0.260814402261038, 'sensitivity_std': 0.026278254107860666, 'specificity_mean': 0.9936039792254308, 'specificity_std': 0.001607758171108381}\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, recall_score, precision_score\n","from scipy.sparse import hstack, csr_matrix\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","# Load dataset\n","data = df_all\n","\n","# Preprocess posts\n","def preprocess_posts(posts):\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = word_tokenize(posts.lower())\n","    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]\n","    return ' '.join(filtered_tokens)\n","\n","# Function to calculate severity scores based on ADHD symptoms\n","def calculate_severity_scores(text):\n","    severity_mapping = {\n","        \"not at all\": 0,\n","        \"somewhat\": 1,\n","        \"pretty much\": 2,\n","        \"very much\": 3\n","    }\n","    symptoms = {\n","        \"attention\": [\"careless\", \"inattentive\", \"distracted\"],\n","        \"hyperactivity\": [\"restless\", \"fidgety\"],\n","        \"impulsivity\": [\"interrupts\", \"impatient\"]\n","    }\n","    score = 0\n","    words = text.split()\n","    for word in words:\n","        for group, keywords in symptoms.items():\n","            if any(keyword in word for keyword in keywords):\n","                score += severity_mapping.get(word, 0)\n","    return score\n","\n","# Process text data and compute scores\n","data['processed_posts'] = data['selected_posts'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n","data['processed_posts'] = data['processed_posts'].apply(preprocess_posts)\n","data['severity_scores'] = data['processed_posts'].apply(calculate_severity_scores)\n","\n","# Extract features using CountVectorizer\n","vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n","X_text_features = vectorizer.fit_transform(data['processed_posts'])\n","X_severity_scores = np.array(data['severity_scores']).reshape(-1, 1)\n","\n","# Combine features into one matrix and convert to CSR format\n","X = hstack([X_text_features, X_severity_scores]).tocsr()\n","\n","# Define labels\n","y = data['diseases'].apply(lambda diseases: 'adhd' in [d.lower() for d in diseases]).values\n","\n","# Initialize Logistic Regression\n","classifier = LogisticRegression()\n","\n","# Perform 5-fold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","f1_scores, sensitivities, specificities = [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Train the classifier\n","    classifier.fit(X_train, y_train)\n","\n","    # Predict outcomes\n","    y_pred = classifier.predict(X_test)\n","\n","    # Compute metrics\n","    f1_scores.append(f1_score(y_test, y_pred))\n","    sensitivities.append(recall_score(y_test, y_pred))\n","    specificities.append(recall_score(y_test, y_pred, pos_label=False))\n","\n","# Compute and display metrics\n","metrics = {\n","    \"f1\": f\"{np.mean(f1_scores):.4f}±{np.std(f1_scores):.4f}\",\n","    \"sensitivity\": f\"{np.mean(sensitivities):.4f}±{np.std(sensitivities):.4f}\",\n","    \"specificity\": f\"{np.mean(specificities):.4f}±{np.std(specificities):.4f}\"\n","}\n","print(\"Metrics:\", metrics)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l817zsuNk3BD","executionInfo":{"status":"ok","timestamp":1717573874484,"user_tz":600,"elapsed":11641319,"user":{"displayName":"Khai-in Lim","userId":"05224726267173946597"}},"outputId":"8941aa2d-ad22-4bd2-a76e-bf447f690942"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Metrics: {'f1': '0.4144±0.0336', 'sensitivity': '0.3968±0.0303', 'specificity': '0.9492±0.0045'}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]}]}