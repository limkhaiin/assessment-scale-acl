{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n",
        "      raw_train_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n",
        "      raw_test_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n",
        "      raw_val_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "LgxAGx4ooLMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.DataFrame(raw_train_data)\n",
        "test_data=pd.DataFrame(raw_test_data)\n",
        "val_data=pd.DataFrame(raw_val_data)"
      ],
      "metadata": {
        "id": "HtlnbL8_oMPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_all = pd.concat([train_data, val_data], axis=0)\n",
        "df_all = pd.concat([df_all, test_data], axis=0)"
      ],
      "metadata": {
        "id": "LSGGjkNooNje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, f1_score, make_scorer\n",
        "from sklearn.utils import resample\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text_list):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_text = []\n",
        "    text = ' '.join(text_list) if isinstance(text_list, list) else text_list\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def calculate_assessment_scores(text):\n",
        "    keywords = {\n",
        "        1: ['limit food', 'diet', 'eat less'],\n",
        "        2: ['not eating', 'skipping meals', 'fasting'],\n",
        "        3: ['obsessed with food', 'focus on calories'],\n",
        "        4: ['worried about weight', 'shape concern'],\n",
        "        5: ['fear of weight gain', 'avoid weight gain'],\n",
        "        6: ['desire to lose weight', 'weight loss'],\n",
        "        7: ['self-induced vomiting', 'use of laxatives'],\n",
        "        8: ['excessive exercise', 'compulsive workout'],\n",
        "        9: ['loss of control eating', 'binge eating'],\n",
        "        10: ['large amount of food', 'overeating'],\n",
        "        11: ['self-worth by weight', 'body image'],\n",
        "        12: ['dissatisfied with weight', 'unhappy with body']\n",
        "    }\n",
        "    scores = np.zeros(len(keywords))\n",
        "    for key, phrases in keywords.items():\n",
        "        for phrase in phrases:\n",
        "            if phrase in text:\n",
        "                scores[key-1] = 1\n",
        "    return scores\n",
        "\n",
        "# Load your data here\n",
        "combined_data = df_all\n",
        "combined_data['processed_posts'] = combined_data['selected_posts'].apply(preprocess_text)\n",
        "\n",
        "# Setup TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=1)\n",
        "X_text = vectorizer.fit_transform(combined_data['processed_posts'])\n",
        "\n",
        "# Calculate and prepare assessment features\n",
        "combined_data['assessment_scores'] = combined_data['processed_posts'].apply(calculate_assessment_scores)\n",
        "assessment_features = np.vstack(combined_data['assessment_scores'])\n",
        "assessment_features_csr = csr_matrix(assessment_features)  # Convert to CSR format\n",
        "\n",
        "# Combine text features with assessment scores\n",
        "X_combined = hstack([X_text, assessment_features_csr]).tocsr()  # Ensure it's in CSR format\n",
        "\n",
        "y = combined_data['diseases'].apply(lambda x: 1 if 'eating' in x else 0).values\n",
        "\n",
        "# Define the logistic regression model with oversampling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "pipeline = make_pipeline(RandomOverSampler(random_state=42), model)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, make_scorer\n",
        "# Metrics setup\n",
        "scoring = {\n",
        "    'f1': 'f1',\n",
        "    'sensitivity': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1] / (confusion_matrix(y_true, y_pred)[1, 1] + confusion_matrix(y_true, y_pred)[1, 0])),\n",
        "    'specificity': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0] / (confusion_matrix(y_true, y_pred)[0, 0] + confusion_matrix(y_true, y_pred)[0, 1]))\n",
        "}\n",
        "\n",
        "# Bootstrapping loop\n",
        "n_bootstraps = 100\n",
        "f1_scores, sensitivities, specificities = [], [], []\n",
        "for _ in range(n_bootstraps):\n",
        "    indices = resample(np.arange(len(y)), replace=True)\n",
        "    X_sample, y_sample = X_combined[indices], y[indices]\n",
        "\n",
        "    pipeline.fit(X_sample, y_sample)\n",
        "    y_pred = pipeline.predict(X_sample)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_sample, y_pred).ravel()\n",
        "    f1_scores.append(f1_score(y_sample, y_pred))\n",
        "    sensitivities.append(tp / (tp + fn))\n",
        "    specificities.append(tn / (tn + fp))\n",
        "\n",
        "# Calculate mean and standard deviation for metrics\n",
        "print(\"Bootstrap Results:\")\n",
        "print(f\"F1 Score: Mean = {np.mean(f1_scores):.4f}, Std = {np.std(f1_scores):.4f}\")\n",
        "print(f\"Sensitivity: Mean = {np.mean(sensitivities):.4f}, Std = {np.std(sensitivities):.4f}\")\n",
        "print(f\"Specificity: Mean = {np.mean(specificities):.4f}, Std = {np.std(specificities):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akDbAeQIqvrp",
        "outputId": "5fc84841-6698-49e6-c66a-78be313c60af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap Results:\n",
            "F1 Score: Mean = 0.5332, Std = 0.0235\n",
            "Sensitivity: Mean = 1.0000, Std = 0.0000\n",
            "Specificity: Mean = 0.9907, Std = 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, make_scorer, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text_list):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_text = []\n",
        "    text = ' '.join(text_list)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def calculate_assessment_scores(text):\n",
        "    keywords = {\n",
        "        1: ['limit food', 'diet', 'eat less'],\n",
        "        2: ['not eating', 'skipping meals', 'fasting'],\n",
        "        3: ['obsessed with food', 'focus on calories'],\n",
        "        4: ['worried about weight', 'shape concern'],\n",
        "        5: ['fear of weight gain', 'avoid weight gain'],\n",
        "        6: ['desire to lose weight', 'weight loss'],\n",
        "        7: ['self-induced vomiting', 'use of laxatives'],\n",
        "        8: ['excessive exercise', 'compulsive workout'],\n",
        "        9: ['loss of control eating', 'binge eating'],\n",
        "        10: ['large amount of food', 'overeating'],\n",
        "        11: ['self-worth by weight', 'body image'],\n",
        "        12: ['dissatisfied with weight', 'unhappy with body']\n",
        "    }\n",
        "\n",
        "    scores = {key: 0 for key in keywords}\n",
        "    for key, phrases in keywords.items():\n",
        "        for phrase in phrases:\n",
        "            if phrase in text:\n",
        "                scores[key] = 1  # You can modify the scoring logic as needed\n",
        "    return scores\n",
        "\n",
        "combined_data = df_all\n",
        "\n",
        "# Preprocess and calculate scores\n",
        "combined_data['processed_posts'] = combined_data['selected_posts'].apply(preprocess_text)\n",
        "combined_data['assessment_scores'] = combined_data['processed_posts'].apply(calculate_assessment_scores)\n",
        "\n",
        "# Extract features from assessment scores\n",
        "for key in combined_data['assessment_scores'].iloc[0].keys():\n",
        "    combined_data[key] = combined_data['assessment_scores'].apply(lambda x: x[key])\n",
        "\n",
        "# Setup TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=1)\n",
        "X_text = vectorizer.fit_transform(combined_data['processed_posts'])\n",
        "\n",
        "# Combine text features with assessment scores\n",
        "assessment_features = np.array(combined_data[list(combined_data['assessment_scores'].iloc[0].keys())])\n",
        "X_combined = hstack([X_text, assessment_features])\n",
        "\n",
        "y = combined_data['diseases'].apply(lambda x: 1 if 'eating' in x else 0)\n",
        "\n",
        "# Define the logistic regression model with oversampling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "pipeline = make_pipeline(RandomOverSampler(random_state=42), model)\n",
        "\n",
        "# Metrics and cross-validation\n",
        "scoring = {'f1': 'f1', 'sensitivity': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1] / (confusion_matrix(y_true, y_pred)[1, 1] + confusion_matrix(y_true, y_pred)[1, 0])),\n",
        "           'specificity': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0] / (confusion_matrix(y_true, y_pred)[0, 0] + confusion_matrix(y_true, y_pred)[0, 1]))}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "results = cross_validate(pipeline, X_combined, y, cv=cv, scoring=scoring)\n",
        "\n",
        "# Display results\n",
        "print(\"Cross-Validation Results:\")\n",
        "for metric in ['f1', 'sensitivity', 'specificity']:\n",
        "    mean_score = np.mean(results[f'test_{metric}'])\n",
        "    std_score = np.std(results[f'test_{metric}'])\n",
        "    print(f\"{metric.capitalize()}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvJgP0a99B4z",
        "outputId": "bc8e2ebf-3daf-48ea-9031-5986fd4e13bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Results:\n",
            "F1: Mean = 0.3306, Std = 0.2112\n",
            "Sensitivity: Mean = 0.4058, Std = 0.0569\n",
            "Specificity: Mean = 0.9562, Std = 0.0766\n"
          ]
        }
      ]
    }
  ]
}