{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WERMMTHzPDyA"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n",
        "      raw_train_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n",
        "      raw_test_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n",
        "      raw_val_data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.DataFrame(raw_train_data)\n",
        "test_data=pd.DataFrame(raw_test_data)\n",
        "val_data=pd.DataFrame(raw_val_data)"
      ],
      "metadata": {
        "id": "7meuv1WNPQrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([train_data, val_data], axis=0)\n",
        "df_all = pd.concat([df_all, test_data], axis=0)"
      ],
      "metadata": {
        "id": "PhWJcMMoPRvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK data is available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your dataset\n",
        "data = df_all\n",
        "\n",
        "# More refined schizophrenia-related keywords\n",
        "schizophrenia_keywords = [\n",
        "    'hallucinations', 'delusions', 'disorganized speech',\n",
        "    'negative symptoms', 'catatonia', 'thought disorder',\n",
        "    'paranoia', 'auditory hallucinations', 'visual hallucinations',\n",
        "    'olfactory hallucinations', 'tactile hallucinations',\n",
        "    'delusion of grandeur', 'delusion of persecution',\n",
        "    'thought insertion', 'thought broadcasting', 'withdrawal',\n",
        "    'anhedonia', 'alogia', 'avolition'\n",
        "]\n",
        "\n",
        "# Function to process text and match keywords\n",
        "def analyze_posts_for_schizophrenia(posts, keywords):\n",
        "    total_scores = []\n",
        "    distress_scores = []\n",
        "    for post_list in posts:\n",
        "        # Combine all posts in the list into a single string\n",
        "        post = ' '.join(post_list).lower()\n",
        "        tokens = word_tokenize(post)\n",
        "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "        # Simple keyword matching for demonstration\n",
        "        keyword_count = sum(1 for word in tokens if word in keywords)\n",
        "        total_scores.append(min(keyword_count, 21))  # Cap the total score\n",
        "        distress_scores.append(min(keyword_count * 3, 105))  # Cap the distress score\n",
        "    return total_scores, distress_scores\n",
        "\n",
        "# Diagnosis criteria adjustment for improved sensitivity and specificity\n",
        "def diagnose_schizophrenia(total_score, distress_score):\n",
        "    if total_score >= 5 and distress_score >= 15:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Balanced)'\n",
        "    elif total_score >= 5:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Maximizing Sensitivity)'\n",
        "    elif distress_score >= 15:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Maximizing Specificity)'\n",
        "    else:\n",
        "        return 'No Psychotic Spectrum Diagnosis'\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "f1_scores = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "for train_index, test_index in kf.split(data):\n",
        "    train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
        "\n",
        "    # Analyze the posts for schizophrenia symptoms\n",
        "    train_scores, train_distress = analyze_posts_for_schizophrenia(train_data['selected_posts'], schizophrenia_keywords)\n",
        "    test_scores, test_distress = analyze_posts_for_schizophrenia(test_data['selected_posts'], schizophrenia_keywords)\n",
        "\n",
        "    # Update data to include scores\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "    train_data['schizophrenia_total_score'] = train_scores\n",
        "    train_data['schizophrenia_distress_score'] = train_distress\n",
        "    test_data['schizophrenia_total_score'] = test_scores\n",
        "    test_data['schizophrenia_distress_score'] = test_distress\n",
        "\n",
        "    # Diagnose based on the criteria\n",
        "    train_data['diagnosis'] = [diagnose_schizophrenia(score, distress) for score, distress in zip(train_data['schizophrenia_total_score'], train_data['schizophrenia_distress_score'])]\n",
        "    test_data['diagnosis'] = [diagnose_schizophrenia(score, distress) for score, distress in zip(test_data['schizophrenia_total_score'], test_data['schizophrenia_distress_score'])]\n",
        "\n",
        "    # Convert 'diseases' column into a binary indicator for schizophrenia presence\n",
        "    true_labels = test_data['diseases'].apply(lambda x: 'schizophrenia' in x).astype(int)\n",
        "    pred_labels = test_data['diagnosis'].apply(lambda x: 'Ultra High Risk/Psychotic Syndrome' in x).astype(int)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\n",
        "\n",
        "    # Calculate sensitivity (recall) and specificity\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "\n",
        "    # Append metrics\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate the average and standard deviation of performance metrics\n",
        "average_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "\n",
        "average_sensitivity = np.mean(sensitivities)\n",
        "std_sensitivity = np.std(sensitivities)\n",
        "\n",
        "average_specificity = np.mean(specificities)\n",
        "std_specificity = np.std(specificities)\n",
        "\n",
        "# Print results with the specified format\n",
        "print(f'Average F1 Score: {average_f1:.2f}±{std_f1:.3f}')\n",
        "print(f'Average Sensitivity: {average_sensitivity:.2f}±{std_sensitivity:.3f}')\n",
        "print(f'Average Specificity: {average_specificity:.2f}±{std_specificity:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDDtG62K8mp",
        "outputId": "013ae8e1-196e-4c93-df26-1b139b180c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.22±0.019\n",
            "Average Sensitivity: 0.16±0.027\n",
            "Average Specificity: 1.00±0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "\n",
        "# Define the function to analyze posts and score them\n",
        "def analyze_posts_for_schizophrenia(posts, keywords):\n",
        "    total_scores = []\n",
        "    distress_scores = []\n",
        "    for post in posts:\n",
        "        if isinstance(post, list):\n",
        "            post = ' '.join(post)\n",
        "        elif isinstance(post, str):\n",
        "            post = post\n",
        "        else:\n",
        "            continue  # Skip if the post is neither a list nor a string\n",
        "\n",
        "        keyword_count = sum(1 for keyword in keywords if keyword.lower() in post.lower())\n",
        "        total_scores.append(min(keyword_count, 21))  # Total score is capped at 21\n",
        "        distress_scores.append(min(keyword_count * 3, 105))  # Distress score is capped at 105 (21 items * 5 points)\n",
        "    return total_scores, distress_scores\n",
        "\n",
        "# Define the diagnosis criteria\n",
        "def diagnose_schizophrenia(total_score, distress_score):\n",
        "    if total_score >= 1 and distress_score >= 6:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Balanced)'\n",
        "    elif total_score >= 1:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Maximizing Sensitivity)'\n",
        "    elif total_score >= 6:\n",
        "        return 'Ultra High Risk/Psychotic Syndrome (Maximizing Specificity)'\n",
        "    else:\n",
        "        return 'No Psychotic Spectrum Diagnosis'\n",
        "\n",
        "# Number of bootstrap samples\n",
        "n_bootstraps = 100\n",
        "\n",
        "# Initialize lists to store metrics for each bootstrap sample\n",
        "bootstrap_f1_scores = []\n",
        "bootstrap_sensitivities = []\n",
        "bootstrap_specificities = []\n",
        "\n",
        "# Perform bootstrapped sampling\n",
        "for _ in range(n_bootstraps):\n",
        "    # Resample the test data with replacement\n",
        "    sample_data = resample(test_data, replace=True, n_samples=len(test_data), random_state=None)\n",
        "\n",
        "    # Analyze the posts for schizophrenia symptoms\n",
        "    sample_data['schizophrenia_total_score'], sample_data['schizophrenia_distress_score'] = analyze_posts_for_schizophrenia(sample_data['selected_posts'], schizophrenia_keywords)\n",
        "\n",
        "    # Diagnose based on the criteria\n",
        "    sample_data['diagnosis'] = sample_data.apply(lambda row: diagnose_schizophrenia(row['schizophrenia_total_score'], row['schizophrenia_distress_score']), axis=1)\n",
        "\n",
        "    # Convert 'diseases' column into a binary indicator for schizophrenia presence\n",
        "    true_labels = sample_data['diseases'].apply(lambda x: 'schizophrenia' in x).astype(int)\n",
        "    pred_labels = (sample_data['diagnosis'].str.contains('Ultra High Risk/Psychotic Syndrome')).astype(int)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "    bootstrap_f1_scores.append(f1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\n",
        "\n",
        "    # Calculate sensitivity (recall) and specificity\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "\n",
        "    # Append metrics\n",
        "    bootstrap_sensitivities.append(sensitivity)\n",
        "    bootstrap_specificities.append(specificity)\n",
        "\n",
        "# Calculate the average and standard deviation of performance metrics\n",
        "average_f1 = np.mean(bootstrap_f1_scores)\n",
        "std_f1 = np.std(bootstrap_f1_scores)\n",
        "\n",
        "average_sensitivity = np.mean(bootstrap_sensitivities)\n",
        "std_sensitivity = np.std(bootstrap_sensitivities)\n",
        "\n",
        "average_specificity = np.mean(bootstrap_specificities)\n",
        "std_specificity = np.std(bootstrap_specificities)\n",
        "\n",
        "# Print results with the specified format\n",
        "print(f'Average F1 Score: {average_f1:.2f}±{std_f1:.3f}')\n",
        "print(f'Average Sensitivity: {average_sensitivity:.2f}±{std_sensitivity:.3f}')\n",
        "print(f'Average Specificity: {average_specificity:.2f}±{std_specificity:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhyIncsPQCTO",
        "outputId": "53b353e9-9eb4-4452-bd44-151ee8518068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.02±0.003\n",
            "Average Sensitivity: 0.99±0.015\n",
            "Average Specificity: 0.09±0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, accuracy_score\n",
        "\n",
        "# Assuming df_all is your DataFrame and it's already loaded\n",
        "data = df_all  # This should be replaced with your actual DataFrame variable\n",
        "data['combined_posts'] = data['selected_posts'].apply(lambda x: ' '.join(x).lower())\n",
        "\n",
        "# Schizophrenia keywords from the checklist\n",
        "schizophrenia_keywords = [\n",
        "    'hallucinations', 'delusions', 'disorganized speech', 'negative symptoms',\n",
        "    'catatonia', 'thought disorder', 'paranoia', 'auditory hallucinations',\n",
        "    'visual hallucinations', 'olfactory hallucinations', 'tactile hallucinations',\n",
        "    'delusion of grandeur', 'delusion of persecution', 'thought insertion',\n",
        "    'thought broadcasting', 'withdrawal', 'anhedonia', 'alogia', 'avolition'\n",
        "]\n",
        "\n",
        "# Function to count the occurrences of schizophrenia-related keywords in posts\n",
        "def keyword_count(text, keywords):\n",
        "    return sum(text.count(keyword) for keyword in keywords)\n",
        "\n",
        "# Apply the function to create a new feature\n",
        "data['keyword_score'] = data['combined_posts'].apply(lambda x: keyword_count(x, schizophrenia_keywords))\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = data[['combined_posts', 'keyword_score']]\n",
        "y = data['diseases'].apply(lambda x: 1 if 'schizophrenia' in x else 0)  # Adjust depending on how schizophrenia is labeled\n",
        "\n",
        "# Preprocessing and Model Pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('txt', CountVectorizer(stop_words='english'), 'combined_posts'),\n",
        "        ('num', 'passthrough', ['keyword_score'])\n",
        "    ])\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))  # Using 'liblinear' for binary classification\n",
        "])\n",
        "\n",
        "# 5-Fold Cross-Validation Setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    sensitivities.append(sensitivity)\n",
        "\n",
        "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Output the average and standard deviation of scores\n",
        "print(f'Average F1 Score: {np.mean(f1_scores):.2f}±{np.std(f1_scores):.3f}')\n",
        "print(f'Average Sensitivity: {np.mean(sensitivities):.2f}±{np.std(sensitivities):.3f}')\n",
        "print(f'Average Specificity: {np.mean(specificities):.2f}±{np.std(specificities):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NqfBEmnEGm5",
        "outputId": "e728c38e-7a66-44b4-9bd7-9ae301410319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score: 0.22±0.064\n",
            "Average Sensitivity: 0.16±0.045\n",
            "Average Specificity: 1.00±0.001\n"
          ]
        }
      ]
    }
  ]
}