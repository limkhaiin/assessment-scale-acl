{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/train.pkl', 'rb') as f:\n",
        "      raw_train_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/test.pkl', 'rb') as f:\n",
        "      raw_test_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/MDDdataset/symptom_sum_top16/val.pkl', 'rb') as f:\n",
        "      raw_val_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "KliDFwe89Ote"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data=pd.DataFrame(raw_train_data)\n",
        "test_data=pd.DataFrame(raw_test_data)\n",
        "val_data=pd.DataFrame(raw_val_data)"
      ],
      "metadata": {
        "id": "ZnqMAAfu9kmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([train_data, val_data], axis=0)\n",
        "df_all = pd.concat([df_all, test_data], axis=0)"
      ],
      "metadata": {
        "id": "JrC6nKA-91DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G89aVeZm8rTt"
      },
      "outputs": [],
      "source": [
        "ptsd_keywords = {\n",
        "    1: ['memories', 'flashback', 'trauma', 'stressful', 'experience'],\n",
        "    2: ['dreams', 'nightmare', 'stressful'],\n",
        "    3: ['reliving', 'reexperiencing', 'flashback', 'stressful'],\n",
        "    4: ['upset', 'reminder', 'stressful', 'experience'],\n",
        "    5: ['physical', 'reaction', 'reminder', 'stressful', 'heart', 'breathing', 'sweating'],\n",
        "    6: ['avoiding', 'memories', 'thoughts', 'feelings', 'stressful'],\n",
        "    7: ['avoiding', 'reminders', 'stressful', 'people', 'places', 'conversations', 'activities'],\n",
        "    8: ['trouble', 'remembering', 'stressful', 'experience'],\n",
        "    9: ['negative', 'beliefs', 'thoughts', 'bad', 'wrong', 'trust', 'dangerous'],\n",
        "    10: ['blaming', 'self', 'others', 'stressful', 'experience'],\n",
        "    11: ['negative', 'feelings', 'fear', 'horror', 'anger', 'guilt', 'shame'],\n",
        "    12: ['loss', 'interest', 'activities', 'enjoy'],\n",
        "    13: ['feeling', 'distant', 'cut off', 'people'],\n",
        "    14: ['trouble', 'positive', 'feelings', 'happiness', 'love'],\n",
        "    15: ['irritable', 'anger', 'outbursts', 'aggressive'],\n",
        "    16: ['risks', 'harm'],\n",
        "    17: ['superalert', 'watchful', 'guard'],\n",
        "    18: ['jumpy', 'startled'],\n",
        "    19: ['difficulty', 'concentrating'],\n",
        "    20: ['trouble', 'sleeping', 'falling', 'asleep', 'staying', 'asleep']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Preprocess text and calculate PTSD score\n",
        "def preprocess_and_score_posts(posts, keywords):\n",
        "    # Ensure input is a string\n",
        "    posts = str(posts)\n",
        "    posts = re.sub(r'[^a-zA-Z\\s]', '', posts).lower().split()\n",
        "    word_counts = Counter(posts)\n",
        "    score = sum(word_counts[keyword] for keyword_list in keywords.values() for keyword in keyword_list)\n",
        "    return score\n",
        "\n",
        "# Apply the scoring function\n",
        "df_all['ptsd_score'] = df_all['selected_posts'].apply(lambda posts: preprocess_and_score_posts(posts, ptsd_keywords))\n",
        "\n",
        "# Define a ground truth for PTSD based on existing diseases column\n",
        "df_all['ground_truth'] = df_all['diseases'].apply(lambda x: 1 if 'ptsd' in x else 0)\n",
        "\n",
        "# Initialize cross-validation and metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores, sensitivities, specificities = [], [], []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, test_index in kf.split(df_all):\n",
        "    train_data, test_data = df_all.iloc[train_index], df_all.iloc[test_index]\n",
        "\n",
        "    # Threshold for PTSD\n",
        "    threshold = 31\n",
        "\n",
        "    # Predict based on score\n",
        "    predictions = (test_data['ptsd_score'] > threshold).astype(int)\n",
        "\n",
        "    # Compute F1 score, sensitivity, and specificity\n",
        "    f1 = f1_score(test_data['ground_truth'], predictions)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(test_data['ground_truth'], predictions).ravel()\n",
        "    sensitivity = tp / (tp + fn) if tp + fn else 0\n",
        "    specificity = tn / (tn + fp) if tn + fp else 0\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate mean and standard deviation for each metric\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "mean_sensitivity = np.mean(sensitivities)\n",
        "std_sensitivity = np.std(sensitivities)\n",
        "mean_specificity = np.mean(specificities)\n",
        "std_specificity = np.std(specificities)\n",
        "\n",
        "# Print the results in the requested format\n",
        "print(f\"F1 Score: {mean_f1:.2f}±{std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {mean_sensitivity:.2f}±{std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {mean_specificity:.2f}±{std_specificity:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRv1X7IS9Ad0",
        "outputId": "108fff22-cec6-41b5-b5da-131d42b12316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.06±0.005\n",
            "Sensitivity: 0.82±0.032\n",
            "Specificity: 0.63±0.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Preprocess text and calculate PTSD score with improved keyword matching and stemming\n",
        "def preprocess_and_score_posts(posts, keywords):\n",
        "    # Ensure input is a string\n",
        "    posts = str(posts)\n",
        "    # Tokenize and stem words\n",
        "    tokens = word_tokenize(posts.lower())\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "    # Remove non-alphabetical characters\n",
        "    stemmed_words = [re.sub(r'[^a-zA-Z]', '', word) for word in stemmed_words if re.sub(r'[^a-zA-Z]', '', word)]\n",
        "    # Count word occurrences\n",
        "    word_counts = Counter(stemmed_words)\n",
        "    score = sum(word_counts[keyword] for keyword_list in keywords.values() for keyword in keyword_list)\n",
        "    return score\n",
        "\n",
        "\n",
        "# Apply the scoring function\n",
        "df_all['ptsd_score'] = df_all['selected_posts'].apply(lambda posts: preprocess_and_score_posts(posts, ptsd_keywords))\n",
        "\n",
        "# Define a ground truth for PTSD based on existing diseases column\n",
        "df_all['ground_truth'] = df_all['diseases'].apply(lambda x: 1 if 'ptsd' in x else 0)\n",
        "\n",
        "# Initialize cross-validation and metrics\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_scores, sensitivities, specificities = [], [], []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, test_index in kf.split(df_all):\n",
        "    train_data, test_data = df_all.iloc[train_index], df_all.iloc[test_index]\n",
        "\n",
        "    # Adjust threshold based on model tuning\n",
        "    threshold = 31\n",
        "\n",
        "    # Predict based on score\n",
        "    predictions = (test_data['ptsd_score'] > threshold).astype(int)\n",
        "\n",
        "    # Compute F1 score, sensitivity, and specificity\n",
        "    f1 = f1_score(test_data['ground_truth'], predictions)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(test_data['ground_truth'], predictions).ravel()\n",
        "    sensitivity = tp / (tp + fn) if tp + fn else 0\n",
        "    specificity = tn / (tn + fp) if tn + fp else 0\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate mean and standard deviation for each metric\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "mean_sensitivity = np.mean(sensitivities)\n",
        "std_sensitivity = np.std(sensitivities)\n",
        "mean_specificity = np.mean(specificities)\n",
        "std_specificity = np.std(specificities)\n",
        "\n",
        "# Print the results in the requested format\n",
        "print(f\"F1 Score: {mean_f1:.2f}±{std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {mean_sensitivity:.2f}±{std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {mean_specificity:.2f}±{std_specificity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2acxITb4Knyz",
        "outputId": "2f449a1e-9bb8-432d-ccb0-2b56054d65ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.13±0.017\n",
            "Sensitivity: 0.36±0.039\n",
            "Specificity: 0.94±0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Function to preprocess and score posts\n",
        "def preprocess_and_score_posts(posts, keywords):\n",
        "    # Ensure input is a string\n",
        "    posts = str(posts)\n",
        "    posts = re.sub(r'[^a-zA-Z\\s]', '', posts).lower().split()\n",
        "    word_counts = Counter(posts)\n",
        "    score = sum(word_counts[keyword] for keyword_list in keywords.values() for keyword in keyword_list)\n",
        "    return score\n",
        "\n",
        "# Apply the scoring function to test data\n",
        "test_data['ptsd_score'] = test_data['selected_posts'].apply(lambda posts: preprocess_and_score_posts(posts, ptsd_keywords))\n",
        "\n",
        "# Initialize lists to hold the metrics from each bootstrap sample\n",
        "f1_scores, sensitivities, specificities = [], [], []\n",
        "\n",
        "# Number of bootstrap samples\n",
        "n_iterations = 100\n",
        "\n",
        "# Bootstrapping\n",
        "for _ in range(n_iterations):\n",
        "    # Resample the test data\n",
        "    bootstrapped_sample = resample(test_data, replace=True, n_samples=len(test_data), random_state=None)\n",
        "\n",
        "    # Predict based on the PTSD score with threshold of 31\n",
        "    predictions = (bootstrapped_sample['ptsd_score'] > 31).astype(int)\n",
        "\n",
        "    # Normally, you need actual labels to calculate these metrics\n",
        "    # Simulate ground truth for demonstration (replace this with actual data if available)\n",
        "    simulated_truth = np.random.randint(0, 2, len(bootstrapped_sample))\n",
        "\n",
        "    # Calculate F1 score, sensitivity, and specificity\n",
        "    f1 = f1_score(simulated_truth, predictions)\n",
        "    tn, fp, fn, tp = confusion_matrix(simulated_truth, predictions).ravel()\n",
        "    sensitivity = tp / (tp + fn) if tp + fn else 0\n",
        "    specificity = tn / (tn + fp) if tn + fp else 0\n",
        "\n",
        "    # Append the results to the lists\n",
        "    f1_scores.append(f1)\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate the mean and standard deviation for each metric\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "mean_sensitivity = np.mean(sensitivities)\n",
        "std_sensitivity = np.std(sensitivities)\n",
        "mean_specificity = np.mean(specificities)\n",
        "std_specificity = np.std(specificities)\n",
        "\n",
        "# Print the results in the requested format\n",
        "print(f\"F1 Score: {mean_f1:.2f}±{std_f1:.3f}\")\n",
        "print(f\"Sensitivity: {mean_sensitivity:.2f}±{std_sensitivity:.3f}\")\n",
        "print(f\"Specificity: {mean_specificity:.2f}±{std_specificity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLVzE_um9_6L",
        "outputId": "bd9625d1-bcef-4480-ba31-5cc5aa32439c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3c9e8d63177f>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['ptsd_score'] = test_data['selected_posts'].apply(lambda posts: preprocess_and_score_posts(posts, ptsd_keywords))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.43±0.009\n",
            "Sensitivity: 0.38±0.010\n",
            "Specificity: 0.62±0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for value in predictions:\n",
        "  if value==1:\n",
        "    count+=1\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-cD394XA0OJ",
        "outputId": "4e72c9e3-5566-4b94-c965-169233e29eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y55_QeCBBL5D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}